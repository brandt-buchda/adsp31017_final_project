{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Load in Data from GitHub\n",
    "data = pd.read_csv('final_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Director and Writer a single column with comma-separated values\n",
    "data['Team'] = data.apply(lambda row: ', '.join(filter(None, [str(row['director']), str(row['writer'])])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['audience_score', 'tomato_meter','wiki_page', 'director', 'writer'], inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cast'] = (\n",
    "    data['cast']\n",
    "    .str.replace(\";\", \",\", regex=False)\n",
    "    .str.replace(\"and\", \",\", regex=False)\n",
    "    .str.replace(\"with\", \",\", regex=False)\n",
    "    .str.replace(\"voices of \", \",\", regex=False)\n",
    "    .str.replace(\"\\r\\n\", \",\", regex=False)\n",
    "    .str.replace(r\"\\(.*?\\)\", \"\", regex=True)  # Remove text within parentheses\n",
    "    .str.replace(\"Cast:\", \"\", regex=False)        # Remove \"Cast:\"\n",
    ")\n",
    "\n",
    "data['Team'] = (\n",
    "    data['Team']\n",
    "    .str.replace(\";\", \",\", regex=False)\n",
    "    .str.replace(\"and\", \",\", regex=False)\n",
    "    .str.replace(\"with\", \",\", regex=False)\n",
    "    .str.replace(\"voices of \", \",\", regex=False)\n",
    "    .str.replace(\"\\r\\n\", \",\", regex=False)\n",
    "    .str.replace(r\"\\(.*?\\)\", \"\", regex=True)  # Remove text within parentheses\n",
    "    .str.replace(\"Director :\", \"\", regex=False)  # Remove \"Director :\"\n",
    ")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Team'] = data['Team'].fillna('').str.split(r',\\s*')\n",
    "data['cast'] = data['cast'].fillna('').str.split(r',\\s*')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Latent Factor Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import MultiLabelBinarizer\n",
    "#from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Create a one-hot encoded matrix from the list of names\n",
    "#mlb = MultiLabelBinarizer()\n",
    "\n",
    "#name_matrix = mlb.fit_transform(data2['names'])\n",
    "\n",
    "#name_df = pd.DataFrame(name_matrix, columns=mlb.classes_)\n",
    "\n",
    "#actor_counts = name_df.sum(axis=0)\n",
    "\n",
    "# Filter out actors that appear only once (i.e., count == 1)\n",
    "#filtered_name_df = name_df.loc[:, actor_counts > 10]\n",
    "\n",
    "#print(\"One-hot encoded matrix:\")\n",
    "#print(name_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fmatrix = filtered_name_df.values\n",
    "\n",
    "# Alternatively, using .to_numpy()\n",
    "#fmatrix = filtered_name_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Use a higher number of components to inspect explained variance\n",
    "#n_components_initial = 2500\n",
    "#svd = TruncatedSVD(n_components=n_components_initial, random_state=42)\n",
    "\n",
    "#svd.fit_transform(name_matrix)\n",
    "\n",
    "#explained_variance = svd.explained_variance_ratio_\n",
    "#cumulative_variance = explained_variance.cumsum()\n",
    "\n",
    "# Plot the cumulative explained variance (Scree Plot)\n",
    "#plt.figure(figsize=(8, 4))\n",
    "#plt.plot(range(1, n_components_initial + 1), cumulative_variance, marker='o')\n",
    "#plt.xlabel('Number of Components')\n",
    "#plt.ylabel('Cumulative Explained Variance')\n",
    "#plt.title('Scree Plot for TSVD Components')\n",
    "#plt.axhline(y=0.8, color='r', linestyle='--', label='80% Variance Threshold')\n",
    "#plt.legend()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movies = name_df.apply(lambda row: list(name_df.columns[row == 1]), axis=1).tolist()\n",
    "\n",
    "#exclusion_tokens = {\" \", \"Jr.\", \"on\", \"er\", \"ler\", '', 'Am', 'S', 'eras', 'a peet'\n",
    "#}\n",
    "\n",
    "# \n",
    "#   cleaned_movies = [\n",
    "#    [actor for actor in movie if actor not in exclusion_tokens]\n",
    "#    for movie in movies\n",
    "#]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim import corpora, models\n",
    "\n",
    "# Create a dictionary mapping actor names to IDs\n",
    "#dictionary = corpora.Dictionary(cleaned_movies)\n",
    "\n",
    "# Convert each movie into a bag-of-actors representation\n",
    "#corpus = [dictionary.doc2bow(movie) for movie in movies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_topics = 10 \n",
    "#lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Enable notebook display for pyLDAvis\n",
    "#pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare the visualization data\n",
    "#vis_data = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "\n",
    "# Display the visualization inline in your notebook\n",
    "#vis_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#from scipy.sparse import csr_matrix\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume name_df is already defined and is a DataFrame with shape (2958, 4823)\n",
    "# For example, you might have:\n",
    "# name_df = pd.read_csv('your_file.csv')\n",
    "\n",
    "# Convert the DataFrame to a SciPy sparse matrix (CSR format)\n",
    "#sparse_matrix = csr_matrix(name_df.values)\n",
    "\n",
    "# Define a range of SVD components to test (e.g., from 10 to 200 in steps of 10)\n",
    "#component_range = range(100, 1000, 100)\n",
    "#cumulative_explained = []\n",
    "\n",
    "# Iterate over the range of components and compute cumulative explained variance\n",
    "#for n_components in component_range:\n",
    "#    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "#    svd.fit(sparse_matrix)\n",
    "#    explained = np.sum(svd.explained_variance_ratio_)\n",
    "#    cumulative_explained.append(explained)\n",
    "#   print(f\"n_components: {n_components}, Cumulative Explained Variance: {explained:.4f}\")\n",
    "\n",
    "# Plot cumulative explained variance vs. number of components\n",
    "#plt.figure(figsize=(8, 4))\n",
    "#plt.plot(list(component_range), cumulative_explained, marker='o')\n",
    "#plt.xlabel('Number of Components')\n",
    "#plt.ylabel('Cumulative Explained Variance')\n",
    "#plt.title('Optimal Number of SVD Components')\n",
    "#plt.grid(True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from scipy.sparse import csr_matrix\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.preprocessing import normalize\n",
    "#from sklearn.metrics import silhouette_score\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#svd = TruncatedSVD(n_components=400, random_state=42)\n",
    "#movie_features = svd.fit_transform(sparse_matrix)\n",
    "\n",
    "# Normalize the feature vectors\n",
    "#movie_features = normalize(movie_features)\n",
    "\n",
    "# Define a range of cluster numbers to try\n",
    "#cluster_range = range(2, 25)  # trying cluster counts from 2 to 15\n",
    "#silhouette_scores = []\n",
    "\n",
    "#for n_clusters in cluster_range:\n",
    "#    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "#    cluster_labels = kmeans.fit_predict(movie_features)\n",
    "#    score = silhouette_score(movie_features, cluster_labels)\n",
    "#    silhouette_scores.append(score)\n",
    "#    print(f\"Number of clusters: {n_clusters} - Silhouette Score: {score:.4f}\")\n",
    "\n",
    "# Plot silhouette scores to visually inspect the optimal cluster count\n",
    "#plt.figure(figsize=(8, 4))\n",
    "#plt.plot(list(cluster_range), silhouette_scores, marker='o')\n",
    "#plt.title(\"Silhouette Score vs. Number of Clusters\")\n",
    "#plt.xlabel(\"Number of Clusters\")\n",
    "#plt.ylabel(\"Silhouette Score\")\n",
    "#plt.xticks(list(cluster_range))\n",
    "#plt.grid(True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse_matrix = csr_matrix(name_df.values)\n",
    "\n",
    "# Define a range of SVD components to test\n",
    "#component_options = [50, 100, 200, 400,500,600, 700, 800, 900]\n",
    "#sil_scores = {}\n",
    "\n",
    "#for n_components in component_options:\n",
    "#    svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "#    features = svd.fit_transform(sparse_matrix)\n",
    "#    features = normalize(features)\n",
    "    \n",
    "    # Use a fixed number of clusters; you can adjust based on your earlier analysis\n",
    "#    kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "    labels = kmeans.fit_predict(features)\n",
    "    \n",
    " #   score = silhouette_score(features, labels)\n",
    " #   sil_scores[n_components] = score\n",
    " #   print(f\"n_components: {n_components} - Silhouette Score: {score:.4f}\")\n",
    "\n",
    "# Plot the silhouette scores against the number of components\n",
    "#plt.figure(figsize=(8, 4))\n",
    "#plt.plot(list(sil_scores.keys()), list(sil_scores.values()), marker='o')\n",
    "#plt.xlabel('Number of SVD Components')\n",
    "#plt.ylabel('Silhouette Score')\n",
    "#plt.title('Silhouette Score vs. SVD Components')\n",
    "#plt.grid(True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Mapped to Team Members (Cast, Director, Writer) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Review\n",
    "reviews = pd.read_csv('Rotten/rotten_tomatoes_movie_reviews.csv')\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Movies\n",
    "movies = pd.read_csv('Rotten/rotten_tomatoes_movies.csv')\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge movies and reviews by 'id'\n",
    "merged_df = pd.merge(movies, reviews, on='id', how='inner')\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique names from the TEAM OR CAST column\n",
    "#unique_names = pd.Series([name for sublist in data['cast'].apply(lambda x: list(set(x))).tolist() for name in sublist]).unique()\n",
    "#unique_names = unique_names[unique_names != '']\n",
    "\n",
    "unique_names = pd.Series([name for sublist in data['Team'].apply(lambda x: list(set(x))).tolist() for name in sublist]).unique()\n",
    "unique_names = unique_names[unique_names != '']\n",
    "\n",
    "unique_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and clean names\n",
    "unique_names = pd.Series(unique_names).drop_duplicates().str.strip()\n",
    "sorted_unique_names = sorted([name for name in unique_names if name and len(name.split()) >= 2 and len(name) > 3])\n",
    "len(sorted_unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_unique_names2 = [name for name in sorted_unique_names if len(name.split()[0]) != 1 and len(name.split()[-1]) > 1]\n",
    "len(sorted_unique_names2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_names = [name for name in sorted_unique_names2 if len(name.split()) < 4]\n",
    "len(final_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DONT RUN THIS CELL\n",
    "\n",
    "# Create a list to store the reviews for each actor\n",
    "reviews = []\n",
    "\n",
    "# Compile regex pattern once for efficiency\n",
    "name_patterns = {name: re.compile(r'\\b' + re.escape(name) + r'\\b', re.IGNORECASE) for name in final_names}\n",
    "\n",
    "# Iterate through each review in merged_df\n",
    "for index, row in merged_df.iterrows():\n",
    "    print(f\"Processing row {index} ({(index / 1469543) * 100:.2f}%)\")\n",
    "    review_text = row['reviewText']\n",
    "    if pd.isna(review_text):\n",
    "        continue\n",
    "    seen_review_ids = set()\n",
    "    # Check each name pattern against the review text\n",
    "    for name, pattern in name_patterns.items():\n",
    "        if pattern.search(review_text) and row['reviewId'] not in seen_review_ids:\n",
    "            reviews.append({\n",
    "                'Name': name, \n",
    "                'Review': review_text, \n",
    "                'reviewId': row['reviewId'], \n",
    "                'creationDate': row['creationDate'],\n",
    "                'audienceScore': row['audienceScore'],\n",
    "                'tomatoMeter': row['tomatoMeter']\n",
    "            })\n",
    "            seen_review_ids.add(row['reviewId'])\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "#actor_reviews_df = pd.DataFrame(reviews)\n",
    "team_reviews_df = pd.DataFrame(reviews)\n",
    "#actor_reviews_df.head()\n",
    "team_reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_reviews_df.sort_values('Name')\n",
    "team_reviews_df.sort_values('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#actor_reviews_df['Name'].nunique()\n",
    "team_reviews_df['Name'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "#actor_reviews_df.to_csv('actor_reviews.csv', index=False)\n",
    "team_reviews_df.to_csv('team_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a JSON file\n",
    "#import json\n",
    "\n",
    "#with open('actor_reviews.json', 'w') as f:\n",
    "#    json.dump(actor_reviews, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in Reviews - ran prior in above cells\n",
    "dat = pd.read_csv('actor_reviews.csv')\n",
    "#team = pd.read_csv('team_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = dat.drop_duplicates()\n",
    "#team1 = team.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dat\n",
    "#del team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine dat1 and team1\n",
    "#combined_dat = pd.concat([dat1, team1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_dat = combined_dat.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate Sentiment Scores for each Actor Review\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "dat1['Sentiment'] = dat1['Review'].apply(lambda review: analyzer.polarity_scores(review))\n",
    "#team1['Sentiment'] = team1['Review'].apply(lambda review: analyzer.polarity_scores(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Main Score \n",
    "dat1['compound'] = dat1['Sentiment'].apply(lambda sentiment: sentiment['compound'])\n",
    "dat1 = dat1.sort_values('Name').reset_index(drop=True)\n",
    "\n",
    "#team1['compound'] = team1['Sentiment'].apply(lambda sentiment: sentiment['compound'])\n",
    "#team1 = team1.sort_values('Name').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1['creationDate'] = pd.to_datetime(dat1['creationDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['release_year'] = pd.to_datetime(data['release_year'].astype(str) + '-01-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expand Data by Cast Members\n",
    "movie_exploded = data.explode(['cast'])\n",
    "#movie_exploded = movie_exploded.explode('Team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Title Volume\n",
    "movie_exploded['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_exploded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with Final Data\n",
    "total = movie_exploded.merge(dat1,left_on='cast', right_on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del movie_exploded\n",
    "del dat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.rename(columns={\n",
    "    'audienceScore': 'Cast_audienceScore',\n",
    "    'tomatoMeter': 'Cast_tomatoMeter',\n",
    "    'Sentiment': 'Cast_Sentiment',\n",
    "    'compound': 'Cast_compound',\n",
    "    'Review': 'Cast_Review',\n",
    "    'reviewId': 'Cast_reviewId',\n",
    "    'creationDate': 'Cast_creationDate'\n",
    "}, inplace=True)\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Title Overlap\n",
    "total['title'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in the 'total' DataFrame\n",
    "total.drop(columns=['Cast_Sentiment'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in the 'total' DataFrame\n",
    "total.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Key for Aggregation\n",
    "total['movie_id'] = total['title'] + '_' + total['release_year'].dt.year.astype(str)\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'movie_id', aggregate metrics and keep all other columns. Make sure to filter for point in time data\n",
    "aggregated_data = total.groupby('movie_id').apply(lambda group: pd.Series({\n",
    "    #'Team_Review_Score_sum': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_compound'].sum() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Team_Review_Score_mean': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_compound'].mean() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Team_Audience_Score_Mean': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_audienceScore'].mean() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Team_Tomato_Meter_Mean': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_tomatoMeter'].mean() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Team_Num_reviews': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_reviewId'].nunique() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Cast_Review_Score_sum': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_compound'].sum() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Cast_Review_Score_mean': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_compound'].mean() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Cast_Audience_Score_Mean': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_audienceScore'].mean() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Cast_Tomato_Meter_Mean': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_tomatoMeter'].mean() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Cast_Num_reviews': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_reviewId'].nunique() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    **{col: group[col].iloc[0] for col in total.columns if col not in ['movie_id', 'Cast_compound', 'Cast_Review', 'Cast_reviewId']}\n",
    "})).reset_index()\n",
    "\n",
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data['movie_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sentiment Score Sum vs Box Office Revenue\n",
    "# Filter out box office outliers\n",
    "Q1 = aggregated_data['box_office'].quantile(0.25)\n",
    "Q3 = aggregated_data['box_office'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "filtered_data = aggregated_data[(aggregated_data['box_office'] >= (Q1 - 1.5 * IQR)) & (aggregated_data['box_office'] <= (Q3 + 1.5 * IQR))]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_data['Cast_Review_Score_sum'], filtered_data['box_office'], alpha=0.5)\n",
    "plt.title('Review Sentiment Score Sum vs Box Office Numeric')\n",
    "plt.xlabel('Review Sentiment Score Sum')\n",
    "plt.ylabel('Box Office Numeric')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Sentiment Score Mean vs Box Office Revenue\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_data['Cast_Review_Score_mean'], filtered_data['box_office'], alpha=0.5)\n",
    "plt.title('Review Sentiment Score Mean vs Box Office Numeric')\n",
    "plt.xlabel('Review Sentiment Score Mean')\n",
    "plt.ylabel('Box Office Numeric')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data['Cast_Review_Score_sum'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data['Cast_Review_Score_mean'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Reviews vs Box Office\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(filtered_data['Cast_Num_reviews'], filtered_data['box_office'], alpha=0.5)\n",
    "plt.title('Number of Reviews vs Box Office')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Box Office Numeric')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data['Cast_Num_reviews'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate aggregated_data number of reviews into quantiles\n",
    "aggregated_data['num_reviews_quantiles'] = pd.qcut(aggregated_data['Cast_Num_reviews'], q=5, labels=False)\n",
    "\n",
    "# Check Interaction with Box Office\n",
    "plt.figure(figsize=(10, 6))\n",
    "aggregated_data.boxplot(column='box_office', by='num_reviews_quantiles')\n",
    "plt.title('Box Office Numeric by Number of Reviews Quantiles')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Number of Reviews Quantiles')\n",
    "plt.ylabel('Box Office Revenue')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Separate aggregated_data compound mean into quantiles\n",
    "aggregated_data['compound_mean_quantiles'] = pd.qcut(aggregated_data['Review_Score_mean'], q=5, labels=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "aggregated_data.boxplot(column='box_office', by='compound_mean_quantiles')\n",
    "plt.title('Box Office Numeric by Compound Mean Quantiles')\n",
    "plt.suptitle('')\n",
    "plt.xlabel('Compound Mean Quantiles')\n",
    "plt.ylabel('Box Office Revenue')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Movies that Lost Money\n",
    "aggregated_data[aggregated_data['budget'] > aggregated_data['box_office']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cluster on these metrics\n",
    "matched_metrics = aggregated_data[['movie_id', 'Cast_Review_Score_sum', 'Cast_Review_Score_mean', 'Cast_Audience_Score_Mean', 'Cast_Tomato_Meter_Mean', 'Cast_Num_reviews']]\n",
    "matched_metrics.set_index('movie_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Ensure no NA values in matched_metrics\n",
    "matched_metrics = matched_metrics.dropna()\n",
    "\n",
    "# Normalize the variables\n",
    "scaler = StandardScaler()\n",
    "matched_metrics_scaled = scaler.fit_transform(matched_metrics)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(matched_metrics_scaled)\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "inertia_values = []\n",
    "silhouette_scores = []\n",
    "K = range(2, 11)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(principal_components)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(principal_components, kmeans.labels_))\n",
    "\n",
    "# Plot inertia \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, inertia_values, marker='o')\n",
    "plt.title('Inertia for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette scores \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, silhouette_scores, marker='x')\n",
    "plt.title('Silhouette Score for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering \n",
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(principal_components)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels to the matched_metrics \n",
    "matched_metrics['Cluster'] = labels\n",
    "\n",
    "# Define cluster labels\n",
    "#cluster_labels = {3: 'Blockbuster Ensemble', 2: 'Critically Acclaimed', 0: 'Steady Performers', 1: 'Low Impact Ensemble'}\n",
    "#matched_metrics['Cluster Label'] = matched_metrics['Cluster'].map(cluster_labels)\n",
    "\n",
    "# Plot clusters with labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1], c=matched_metrics['Cluster'], cmap='viridis')\n",
    "plt.title('Clusters of Matched Metrics')\n",
    "plt.xlabel('First Metric')\n",
    "plt.ylabel('Second Metric')\n",
    "plt.grid(True)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "# Add legend with cluster labels\n",
    "handles, _ = scatter.legend_elements()\n",
    "#legend_labels = [cluster_labels[i] for i in range(len(handles))]\n",
    "#plt.legend(handles, legend_labels, title=\"Cluster Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cluster, get description of principal components\n",
    "numeric_columns = matched_metrics.select_dtypes(include='number').columns\n",
    "cluster_descriptions = matched_metrics.groupby('Cluster')[numeric_columns].mean()\n",
    "cluster_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_metrics['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cluster labels\n",
    "cluster_labels = {0: 'Emerging Ensemble', 1: 'Unknown Cast', 2: 'Star Studded Cast', 3: 'Steady Performers'}\n",
    "matched_metrics['Cluster Label'] = matched_metrics['Cluster'].map(cluster_labels)\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "matched_metrics.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1], c=matched_metrics['Cluster'], cmap='viridis')\n",
    "plt.title('Clusters of Matched Metrics')\n",
    "plt.xlabel('First Metric')\n",
    "plt.ylabel('Second Metric')\n",
    "plt.grid(True)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "# Add legend with cluster labels\n",
    "handles, _ = scatter.legend_elements()\n",
    "legend_labels = [cluster_labels[i] for i in range(len(handles))]\n",
    "plt.legend(handles, legend_labels, title=\"Cluster Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the Cluster Label and merge back to aggregated_data\n",
    "aggregated_data = aggregated_data.merge(matched_metrics[['Cluster Label']], left_on='movie_id', right_index=True, how='left')\n",
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name of the column 'Cluster Label' to 'Cast_Cluster_Label'\n",
    "aggregated_data.rename(columns={'Cluster Label': 'Cast_Cluster_Label'}, inplace=True)\n",
    "aggregated_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = pd.read_csv('team_reviews.csv')\n",
    "team1 = team.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1['Sentiment'] = team1['Review'].apply(lambda review: analyzer.polarity_scores(review))\n",
    "team1['compound'] = team1['Sentiment'].apply(lambda sentiment: sentiment['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_aggregated_data = aggregated_data.explode('Team')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_aggregated_data.drop(columns=['Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = exploded_aggregated_data.merge(team1,left_on='Team', right_on='Name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.rename(columns={\n",
    "    'audienceScore': 'Team_audienceScore',\n",
    "    'tomatoMeter': 'Team_tomatoMeter',\n",
    "    'Sentiment': 'Team_Sentiment',\n",
    "    'compound': 'Team_compound',\n",
    "    'Review': 'Team_Review',\n",
    "    'reviewId': 'Team_reviewId',\n",
    "    'creationDate': 'Team_creationDate'\n",
    "}, inplace=True)\n",
    "total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_team_cast = total.groupby('movie_id').apply(lambda group: pd.Series({\n",
    "    'Team_Review_Score_sum': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_compound'].sum() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Team_Review_Score_mean': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_compound'].mean() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Team_Audience_Score_Mean': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_audienceScore'].mean() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Team_Tomato_Meter_Mean': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_tomatoMeter'].mean() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    'Team_Num_reviews': group.loc[group['Team_creationDate'] < group['release_year'], 'Team_reviewId'].nunique() if not group[group['Team_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Cast_Review_Score_sum': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_compound'].sum() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Cast_Review_Score_mean': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_compound'].mean() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Cast_Audience_Score_Mean': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_audienceScore'].mean() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Cast_Tomato_Meter_Mean': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_tomatoMeter'].mean() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    #'Cast_Num_reviews': group.loc[group['Cast_creationDate'] < group['release_year'], 'Cast_reviewId'].nunique() if not group[group['Cast_creationDate'] < group['release_year']].empty else 0,\n",
    "    **{col: group[col].iloc[0] for col in total.columns if col not in ['movie_id', 'Team_compound', 'Team_Review', 'Team_reviewId']}\n",
    "})).reset_index()\n",
    "\n",
    "aggregated_data_team_cast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_metrics_team_cast = aggregated_data_team_cast[['movie_id', 'Team_Review_Score_sum', 'Team_Review_Score_mean', 'Team_Audience_Score_Mean', 'Team_Tomato_Meter_Mean', 'Team_Num_reviews']]\n",
    "matched_metrics_team_cast.set_index('movie_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_metrics_team_cast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_metrics_team_cast= matched_metrics_team_cast.dropna()\n",
    "\n",
    "# Normalize the variables\n",
    "scaler = StandardScaler()\n",
    "matched_metrics_team_cast_scaled = scaler.fit_transform(matched_metrics_team_cast)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(matched_metrics_team_cast_scaled)\n",
    "\n",
    "# Find the optimal number of clusters\n",
    "inertia_values = []\n",
    "silhouette_scores = []\n",
    "K = range(2, 11)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(principal_components)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    silhouette_scores.append(silhouette_score(principal_components, kmeans.labels_))\n",
    "\n",
    "# Plot inertia \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, inertia_values, marker='o')\n",
    "plt.title('Inertia for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot silhouette scores \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(K, silhouette_scores, marker='x')\n",
    "plt.title('Silhouette Score for Different Numbers of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans.fit(principal_components)\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels to the matched_metrics \n",
    "matched_metrics_team_cast['Cluster'] = labels\n",
    "\n",
    "# Define cluster labels\n",
    "#cluster_labels = {3: 'Blockbuster Ensemble', 2: 'Critically Acclaimed', 0: 'Steady Performers', 1: 'Low Impact Ensemble'}\n",
    "#matched_metrics['Cluster Label'] = matched_metrics['Cluster'].map(cluster_labels)\n",
    "\n",
    "# Plot clusters with labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1], c=matched_metrics_team_cast['Cluster'], cmap='viridis')\n",
    "plt.title('Clusters of Matched Metrics')\n",
    "plt.xlabel('First Metric')\n",
    "plt.ylabel('Second Metric')\n",
    "plt.grid(True)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "# Add legend with cluster labels\n",
    "handles, _ = scatter.legend_elements()\n",
    "#legend_labels = [cluster_labels[i] for i in range(len(handles))]\n",
    "#plt.legend(handles, legend_labels, title=\"Cluster Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = matched_metrics_team_cast.select_dtypes(include='number').columns\n",
    "cluster_descriptions = matched_metrics_team_cast.groupby('Cluster')[numeric_columns].mean()\n",
    "cluster_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_metrics_team_cast['Cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cluster labels\n",
    "cluster_labels = {0: 'Unknown Director / Writer', 1: 'Emerging Director / Writer', 2: 'Blockbuster Team', 3: 'Established Team'}\n",
    "matched_metrics_team_cast['Cluster Label'] = matched_metrics_team_cast['Cluster'].map(cluster_labels)\n",
    "\n",
    "# Display the first few rows to verify the cluster labels\n",
    "matched_metrics_team_cast.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(principal_components[:, 0], principal_components[:, 1], c=matched_metrics_team_cast['Cluster'], cmap='viridis')\n",
    "plt.title('Clusters of Matched Metrics')\n",
    "plt.xlabel('First Metric')\n",
    "plt.ylabel('Second Metric')\n",
    "plt.grid(True)\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "# Add legend with cluster labels\n",
    "handles, _ = scatter.legend_elements()\n",
    "legend_labels = [cluster_labels[i] for i in range(len(handles))]\n",
    "plt.legend(handles, legend_labels, title=\"Cluster Labels\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_team_cast = aggregated_data_team_cast.merge(matched_metrics_team_cast[['Cluster Label']], left_on='movie_id', right_index=True, how='left')\n",
    "aggregated_data_team_cast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'Team_Sentiment' column\n",
    "aggregated_data_team_cast = aggregated_data_team_cast.drop(columns=['Team_Sentiment'])\n",
    "\n",
    "# Rename 'Cluster Label' to 'Team_Cluster Label'\n",
    "aggregated_data_team_cast = aggregated_data_team_cast.rename(columns={'Cluster Label': 'Team_Cluster Label'})\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "aggregated_data_team_cast.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_team_cast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINAL DATASET\n",
    "aggregated_data_team_cast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_team_cast.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data_team_cast.to_csv('final_dataset_cam.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
