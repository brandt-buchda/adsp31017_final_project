{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scripts.utilites import column_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_df_with_emotion_scores.csv\", low_memory=False)\n",
    "# display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_merge = pd.read_csv(\"final_dataset_cam.csv\")\n",
    "# print(df_to_merge.columns.to_list())\n",
    "# display(column_stats(df_to_merge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_titles(df, column=\"title\"):\n",
    "    df[column] = df[column].str.lower().str.strip()\n",
    "    df[column] = df[column].apply(lambda x: ' '.join(re.sub(r'[^a-z0-9\\s]', '', x).split()) if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "df_to_merge = standardize_titles(df_to_merge)\n",
    "\n",
    "# Merge only the id_x column from df_merged into df\n",
    "df = df.merge(df_to_merge[['title', 'Team_Review_Score_sum', 'Team_Review_Score_mean', 'Team_Audience_Score_Mean', 'Team_Tomato_Meter_Mean', 'Team_Num_reviews', 'Cast_Review_Score_sum', 'Cast_Review_Score_mean', 'Cast_Audience_Score_Mean', 'Cast_Tomato_Meter_Mean', 'Cast_Num_reviews', 'Cast_creationDate', 'Cast_audienceScore', 'Cast_tomatoMeter', 'Cast_Cluster_Label', 'Team_creationDate', 'Team_audienceScore', 'Team_tomatoMeter', 'Team_Cluster Label']], left_on='title', right_on='title', how='left')\n",
    "\n",
    "# Display column stats\n",
    "# display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(column_stats(df_to_merge))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Create CPI Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fred = Fred(api_key='<YOUR-API-KEY-HERE>')\n",
    "#\n",
    "# cpi_series = fred.get_series('CPIAUCSL', observation_start='1977-01-01', observation_end='2024-12-31')\n",
    "#\n",
    "# cpi_df = pd.DataFrame(cpi_series)\n",
    "# cpi_df.columns = ['CPI']\n",
    "#\n",
    "# cpi_df.to_csv('external/cpi_data.csv')\n",
    "\n",
    "cpi_df = pd.read_csv(\"external/cpi_data.csv\", names=[\"date\", \"cpi\"], skiprows=1)\n",
    "cpi_df[\"date\"] = pd.to_datetime(cpi_df[\"date\"])\n",
    "cpi_df.set_index(\"date\", inplace=True)\n",
    "cpi_yearly_df = cpi_df.resample(\"YE\").mean()\n",
    "cpi_yearly_df[\"year\"] = cpi_yearly_df.index.year\n",
    "cpi_yearly_df.set_index(\"year\", inplace=True)\n",
    "# display(cpi_yearly_df)\n",
    "\n",
    "cpi_yearly_df.to_csv(\"external/cpi_yearly.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.merge(cpi_yearly_df, left_on=\"release_year\", right_index=True, how=\"left\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Create Box Office Gross Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_office_gross_df = pd.read_csv(\"external/box_office_revenue.csv\", low_memory=False)\n",
    "box_office_gross_df.set_index(\"year\", inplace=True)\n",
    "display(box_office_gross_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.merge(box_office_gross_df, left_on=\"release_year\", right_index=True, how=\"left\")\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Create Box Office Adjusted Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest_cpi = cpi_yearly_df[\"cpi\"].iloc[-1]\n",
    "# latest_gross = df[\"box_office_gross\"].iloc[-1]\n",
    "#\n",
    "# df[\"box_office_adjusted\"] = (\n",
    "#     df[\"box_office\"]\n",
    "#     * (latest_cpi / df[\"cpi\"])\n",
    "#     * (latest_gross / df[\"box_office_gross\"])\n",
    "# )\n",
    "#\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Create Budget Adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize the raw_data adjuster\n",
    "# adjuster = DataAdjuster(\n",
    "#     adjust_cpi=False,\n",
    "#     adjust_gross=False,\n",
    "#     apply_boxcox=False,\n",
    "#     cpi_data_path=\"../external_data/cpi_yearly.csv\",\n",
    "#     gross_data_path=\"../external_data/box_office_revenue.csv\"\n",
    "# )\n",
    "#\n",
    "# # Apply adjustments\n",
    "# df = adjuster.apply_adjustments(df, \"box_office\", \"release_year\", \"budget\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest_cpi = cpi_yearly_df[\"cpi\"].iloc[-1]\n",
    "# latest_gross = df[\"box_office_gross\"].iloc[-1]\n",
    "#\n",
    "# df[\"budget_adjusted\"] = (\n",
    "#     df[\"budget\"]\n",
    "#     * (latest_cpi / df[\"cpi\"])\n",
    "#     * (latest_gross / df[\"box_office_gross\"])\n",
    "# )\n",
    "#\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Encode and Impute Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"rating\"].unique())\n",
    "\n",
    "def encode_rating(rating):\n",
    "    rating_map = {\n",
    "        np.nan: 0,\n",
    "        \"pg\": 1,\n",
    "        \"tvpg\": 2,\n",
    "        \"pg-13\": 3,\n",
    "        \"nc-17\": 4,\n",
    "        \"tvma\": 5,\n",
    "        \"r\": 6\n",
    "    }\n",
    "    return rating_map[rating]\n",
    "\n",
    "df[\"rating\"] = df[\"rating\"].apply(lambda x: encode_rating(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Encode Distributor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Encode Belongs to Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"is_in_collection\"] = df[\"belongs_to_collection\"].notna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_eval(value):\n",
    "    \"\"\"Safely evaluate a string representation of a dictionary.\"\"\"\n",
    "    if pd.isna(value):  # Handle NaN values\n",
    "        return None\n",
    "    if isinstance(value, dict):  # Already a dictionary, no need to parse\n",
    "        return value\n",
    "    if not isinstance(value, str):  # If it's not a string, return None\n",
    "        return None\n",
    "    try:\n",
    "        parsed_value = ast.literal_eval(value)\n",
    "        if isinstance(parsed_value, dict):  # Ensure it's a dictionary\n",
    "            return parsed_value\n",
    "    except (ValueError, SyntaxError):  # Catch any parsing errors\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "# Convert the column from string to dictionary safely\n",
    "df[\"belongs_to_collection\"] = df[\"belongs_to_collection\"].apply(safe_eval)\n",
    "\n",
    "# Extract collection ID\n",
    "df[\"collection_id\"] = df[\"belongs_to_collection\"].apply(lambda x: x[\"id\"] if isinstance(x, dict) else None)\n",
    "\n",
    "# Compute the average box office per collection\n",
    "collection_avg_box_office = df.groupby(\"collection_id\")[\"box_office\"].mean()\n",
    "\n",
    "# Map the computed averages back to the DataFrame\n",
    "df[\"collection_box_office_average\"] = df[\"collection_id\"].map(collection_avg_box_office)\n",
    "\n",
    "# Fill NaN values (movies not in a collection) with 0\n",
    "df[\"collection_box_office_average\"] = df[\"collection_box_office_average\"].fillna(0)\n",
    "df.drop(columns=[\"belongs_to_collection\"], inplace=True)\n",
    "\n",
    "df[\"collection_id\"] = df[\"collection_id\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Cast, Writer, Director Contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_contributions(category, n=3):\n",
    "    def compute_contributions(names):\n",
    "        members = [name.strip().title() for name in str(names).split(\",\") if name.strip()]\n",
    "        members = sorted(members, key=lambda x: avg_dict.get(x, float('-inf')), reverse=True)\n",
    "        top_n = [avg_dict.get(member, float('nan')) for member in members[:n]]\n",
    "        return pd.Series(top_n + [float('nan')] * (n - len(top_n)))\n",
    "\n",
    "    avg_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row[category]) or pd.isna(row[\"box_office\"]):\n",
    "            continue\n",
    "        members = [name.strip().title() for name in str(row[category]).split(\",\") if name.strip()]\n",
    "        for member in members:\n",
    "            if member not in avg_dict:\n",
    "                avg_dict[member] = {\"total_revenue\": 0, \"count\": 0}\n",
    "            avg_dict[member][\"total_revenue\"] += row[\"box_office\"]\n",
    "            avg_dict[member][\"count\"] += 1\n",
    "\n",
    "    avg_dict = {member: stats[\"total_revenue\"] / stats[\"count\"] for member, stats in avg_dict.items() if stats[\"count\"] > 0}\n",
    "\n",
    "    col_names = [f\"{category}_top_{i+1}_contrib\" for i in range(n)]\n",
    "    df[col_names] = df[category].apply(compute_contributions)\n",
    "\n",
    "top_contributions(\"cast\", 5)\n",
    "top_contributions(\"director\", 1)\n",
    "top_contributions(\"writer\", 3)\n",
    "top_contributions(\"distributor\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def process_comma_separated_column(df, column_name, top_n=10):\n",
    "    # Step 1: Extract valid values and count occurrences\n",
    "    all_values = []\n",
    "    for values in df[column_name]:\n",
    "        if isinstance(values, str):  # Ensure only strings are processed\n",
    "            all_values.extend([value.strip() for value in values.split(',')])\n",
    "\n",
    "    value_counts = Counter(all_values)\n",
    "\n",
    "    # Step 2: Get the `top_n` most frequent values\n",
    "    top_values = {v for v, _ in value_counts.most_common(top_n)}\n",
    "\n",
    "    # Step 3: Create boolean columns for the top values\n",
    "    for value in top_values:\n",
    "        df[value] = df[column_name].apply(lambda x: isinstance(x, str) and any(v.strip() == value for v in x.split(',')))\n",
    "\n",
    "    # Step 4: Drop the original column\n",
    "    df.drop(columns=[column_name], inplace=True)\n",
    "\n",
    "process_comma_separated_column(df, 'distributor', top_n=10)\n",
    "process_comma_separated_column(df, 'sound_mix', top_n=5)\n",
    "process_comma_separated_column(df, 'rating_contents', top_n=10)\n",
    "process_comma_separated_column(df, 'genre', top_n=10)\n",
    "process_comma_separated_column(df, 'Cast_Cluster_Label', top_n=10)\n",
    "process_comma_separated_column(df, 'Team_Cluster Label', top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"director\", \"writer\", \"Dominant_Emotion\", \"id_x\", \"Cast_creationDate\", \"Team_creationDate\", \"cast\", \"wiki_page\", \"plot\", \"id\", \"audience_score\", \"tomato_meter\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=[\"budget\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../complete_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Assume df is your original DataFrame with at least \"box_office\" and \"release_year\" columns\n",
    "df = df[df[\"box_office\"] > 100000]\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(columns=[\"box_office\", \"title\"])  # Keep all features including release_year\n",
    "y = df[\"box_office\"]  # Adjusted values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate the date column so it won't be affected by scaling\n",
    "date_col = \"release_year\"\n",
    "if date_col in X_train.columns:\n",
    "    X_train_date = X_train[[date_col]]\n",
    "    X_train_scaled = X_train.drop(columns=[date_col])\n",
    "else:\n",
    "    X_train_scaled = X_train.copy()\n",
    "\n",
    "if date_col in X_test.columns:\n",
    "    X_test_date = X_test[[date_col]]\n",
    "    X_test_scaled = X_test.drop(columns=[date_col])\n",
    "else:\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "# Feature scaling (only on non-date features)\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_scaled),\n",
    "                              columns=X_train_scaled.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(scaler.transform(X_test_scaled),\n",
    "                             columns=X_test_scaled.columns, index=X_test.index)\n",
    "\n",
    "# Recombine the date column\n",
    "if date_col in X_train.columns:\n",
    "    X_train = pd.concat([X_train_scaled, X_train_date], axis=1)\n",
    "    X_test = pd.concat([X_test_scaled, X_test_date], axis=1)\n",
    "else:\n",
    "    X_train = X_train_scaled\n",
    "    X_test = X_test_scaled\n",
    "\n",
    "# Define and train the XGBoost model\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"max_depth\": 32,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"n_estimators\": 1000,\n",
    "}\n",
    "model = xgb.XGBRegressor(**xgb_params)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Prepare DataFrames for reversing adjustments\n",
    "# (Make sure the date column is present so that adjuster.reverse_adjustments can merge CPI/gross raw_data)\n",
    "y_test_df = pd.DataFrame(y_test, columns=[\"box_office\"]).assign(release_year=X_test[date_col])\n",
    "y_pred_df = pd.DataFrame({\n",
    "    \"box_office\": y_pred,\n",
    "    \"release_year\": X_test.reset_index(drop=True)[date_col]  # Ensures alignment\n",
    "})\n",
    "\n",
    "\n",
    "# Calculate SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Calculate evaluation metrics on the reversed (original scale) values\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "smape_value = smape(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAPE: {mape:.4%}\")\n",
    "print(f\"SMAPE: {smape_value:.4f}%\")\n",
    "print(f\"R^2 Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plot\n",
    "residuals = y_pred - y_test\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=y_pred, y=residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Zero error reference\n",
    "plt.xlabel(\"Predicted Values\")\n",
    "plt.ylabel(\"Residuals (Actual - Predicted)\")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram of residuals\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.kdeplot(residuals)\n",
    "plt.axvline(x=0, color='r', linestyle='--')  # Zero error reference\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xscale(\"symlog\")\n",
    "plt.title(\"Residuals Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = model.get_booster().get_score(importance_type=\"gain\")  # or \"weight\", \"cover\"\n",
    "\n",
    "# Convert to DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": list(feature_importance.keys()),\n",
    "    \"Importance\": list(feature_importance.values())\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Select top N features\n",
    "N = 15  # Change this to however many top features you want\n",
    "top_features = importance_df.head(N)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(y=top_features[\"Feature\"], x=top_features[\"Importance\"], palette=\"viridis\")\n",
    "plt.xlabel(\"Importance (Gain)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(f\"Top {N} Most Important Features in XGBoost\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
