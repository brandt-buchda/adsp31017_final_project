{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import ast\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from scripts.utilites import column_stats\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('merged_data/merged.csv')\n",
    "\n",
    "display(df[df[\"title\"].str.contains(\"Star Wars\", case=False, na=False)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Limit to Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Combine Duplicate Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Release Date\n",
    "We are just using this for adjusting the box office so just the year is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_cols = [\"release_date_theaters\", \"release_year\", \"release_date\"]\n",
    "\n",
    "display(df[date_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop if they can't be filled via future data mining\n",
    "df.dropna(subset=date_cols + ['wiki_page'], inplace=True, how='all')\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[date_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert columns to just the year\n",
    "df[\"release_date_theaters\"] = pd.to_datetime(df[\"release_date_theaters\"], errors=\"coerce\")\n",
    "df[\"release_date\"] = pd.to_datetime(df[\"release_date\"], errors=\"coerce\")\n",
    "\n",
    "# Convert all columns to just the year\n",
    "df[\"release_date_theaters\"] = df[\"release_date_theaters\"].dt.year\n",
    "df[\"release_date\"] = df[\"release_date\"].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[date_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find rows where at least two different years exist (ignoring NaNs)\n",
    "mismatch_mask = df[[\"release_date_theaters\", \"release_year\", \"release_date\"]].nunique(axis=1, dropna=True) > 1\n",
    "\n",
    "df = df.loc[~mismatch_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"release_year\"] = df[\"release_year\"].fillna(df[\"release_date_theaters\"]).fillna(df[\"release_date\"])\n",
    "df.drop(columns=[\"release_date_theaters\", \"release_date\"] , inplace=True)\n",
    "\n",
    "display(df)\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Box Office"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[[\"box_office\", \"revenue\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop if they can't be filled via future data mining\n",
    "df[[\"box_office\", \"revenue\"]] = df[[\"box_office\", \"revenue\"]].replace(0.0, np.nan)\n",
    "\n",
    "df.dropna(subset=[\"box_office\", \"revenue\"] + ['wiki_page'], inplace=True, how='all')\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "display(df[[\"box_office\", \"revenue\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_revenue(value):\n",
    "    value = str(value)\n",
    "    value = value.replace('$', '').replace(',', '')\n",
    "    if 'M' in value:\n",
    "        return float(value.replace('M', '')) * 1_000_000\n",
    "    elif 'K' in value:\n",
    "        return float(value.replace('K', '')) * 1_000\n",
    "    elif 'B' in value:\n",
    "        return float(value.replace('B', '')) * 1_000_000_000\n",
    "    else:\n",
    "        return float(value)\n",
    "\n",
    "df['box_office'] = df['box_office'].apply(convert_revenue)\n",
    "\n",
    "display(df[[\"box_office\", \"revenue\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"box_office\"] = df[\"revenue\"].fillna(df[\"box_office\"])\n",
    "df.drop(\"revenue\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "display(df)\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the absolute percentage difference\n",
    "mask_mismatch_runtime = (\n",
    "    (df[\"runtime_minutes\"].notna()) & (df[\"runtime\"].notna()) &  # Ensure both values are present\n",
    "    (abs(df[\"runtime_minutes\"] - df[\"runtime\"]) / df[\"runtime\"] > 0.10)  # Check >10% difference\n",
    ")\n",
    "\n",
    "# Display the mismatched rows\n",
    "display(df.loc[mask_mismatch_runtime, [\"runtime_minutes\", \"runtime\", \"title\"]])\n",
    "\n",
    "# Collapse into a single 'runtime' column, prioritizing non-null values\n",
    "df[\"runtime\"] = df[\"runtime\"].fillna(df[\"runtime_minutes\"])\n",
    "\n",
    "# Drop the old 'runtime_minutes' column\n",
    "df.drop(columns=[\"runtime_minutes\"], inplace=True)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "display(df)\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df[\"title\"].str.contains(\"Star Wars\", case=False, na=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop if they can't be filled from other columns\n",
    "df.dropna(subset=[\"genre_x\", \"genre_y\", \"genres\"], inplace=True, how='all')\n",
    "\n",
    "display(df[[\"genre_x\", \"genre_y\", \"genres\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_genres(row):\n",
    "    genres = set()\n",
    "    invalid_values = {\"unknown\", \"nan\", \"\"}\n",
    "\n",
    "    if pd.notna(row[\"genre_x\"]):\n",
    "        genres.update(x.strip().lower() for x in row[\"genre_x\"].split(',') if x.strip().lower() not in invalid_values)\n",
    "    if pd.notna(row[\"genre_y\"]):\n",
    "        genres.update(x.strip().lower() for x in row[\"genre_y\"].split(',') if x.strip().lower() not in invalid_values)\n",
    "\n",
    "    if pd.notna(row[\"genres\"]) and isinstance(row[\"genres\"], list):\n",
    "        genres.update(genre[\"name\"].lower() for genre in row[\"genres\"] if \"name\" in genre and genre[\"name\"].lower() not in invalid_values)\n",
    "\n",
    "    return \", \".join(genres) if genres else None\n",
    "\n",
    "df[\"genre\"] = df.apply(combine_genres, axis=1)\n",
    "df.drop(columns=[\"genre_x\", \"genre_y\", \"genres\"], inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_cols = [\"original_language_x\", \"original_language_y\"]\n",
    "for col in lang_cols:\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_map = {\n",
    "    \"en\": \"english\", \"fr\": \"french\", \"es\": \"spanish\", \"de\": \"german\", \"it\": \"italian\",\n",
    "    \"pt\": \"portuguese\", \"ru\": \"russian\", \"zh\": \"chinese\", \"ja\": \"japanese\", \"ko\": \"korean\",\n",
    "    \"hi\": \"hindi\", \"ar\": \"arabic\", \"bn\": \"bengali\", \"pa\": \"punjabi\", \"ur\": \"urdu\",\n",
    "    \"fa\": \"persian\", \"tr\": \"turkish\", \"pl\": \"polish\", \"nl\": \"dutch\", \"sv\": \"swedish\",\n",
    "    \"fi\": \"finnish\", \"no\": \"norwegian\", \"da\": \"danish\", \"cs\": \"czech\", \"el\": \"greek\",\n",
    "    \"hu\": \"hungarian\", \"ro\": \"romanian\", \"th\": \"thai\", \"vi\": \"vietnamese\",\n",
    "    \"he\": \"hebrew\", \"id\": \"indonesian\", \"uk\": \"ukrainian\", \"xx\": None\n",
    "}\n",
    "\n",
    "def clean_language(lang):\n",
    "    if pd.isna(lang) or lang.lower() in {\"unknown language\", \"nan\", \"\"}:\n",
    "        return None\n",
    "\n",
    "    lang = lang.lower().strip()\n",
    "\n",
    "    lang = re.split(r'\\s*\\(', lang)[0].strip()\n",
    "\n",
    "    return language_map.get(lang, lang)\n",
    "\n",
    "df[\"original_language_x\"] = df[\"original_language_x\"].apply(clean_language)\n",
    "\n",
    "df[\"language\"] = df[\"original_language_x\"].fillna(df[\"original_language_y\"].apply(clean_language))\n",
    "df.drop(columns=[\"original_language_x\", \"original_language_y\", \"spoken_languages\"], inplace=True)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Production Companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"production_companies\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Drop non-english, pre 1977 movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df[\"title\"].str.contains(\"Star Wars\", case=False, na=False)])\n",
    "df = df[df[\"release_year\"] >= 1977]\n",
    "df = df[df[\"language\"].str.contains(\"English\", case=False, na=False)]\n",
    "display(df[df[\"title\"].str.contains(\"Star Wars\", case=False, na=False)])\n",
    "\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Wikipedia Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wikipedia_page(url, session):\n",
    "    \"\"\"Fetch the full HTML content of a Wikipedia page using a session.\"\"\"\n",
    "    if pd.isna(url) or not isinstance(url, str) or not url.startswith(\"http\"):\n",
    "        return None, None  # Ensure both values are returned\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()  # Track request start time\n",
    "        response = session.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        elapsed_time = time.time() - start_time  # Calculate time taken\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return None, elapsed_time  # Return time even on failure\n",
    "\n",
    "        return response.text, elapsed_time  # Store HTML and request time\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")  # Log errors for debugging\n",
    "        return None, None  # Always return a tuple\n",
    "\n",
    "\n",
    "def scrape_wikipedia_pages(df, url_column, output_column):\n",
    "    \"\"\"Scrape Wikipedia pages with a progress bar and dynamic ETA display.\"\"\"\n",
    "    missing_mask = df[output_column].isna()\n",
    "    urls_to_scrape = df.loc[missing_mask, url_column]\n",
    "    times = []  # Store request times for rolling average\n",
    "\n",
    "    session = requests.Session()  # Use a session for efficiency\n",
    "\n",
    "    print(f\"Scraping {len(urls_to_scrape)} pages...\")\n",
    "    progress_bar = tqdm(urls_to_scrape, desc=\"Fetching pages\", unit=\"page\")\n",
    "\n",
    "    for idx, url in enumerate(progress_bar):\n",
    "        html, elapsed_time = fetch_wikipedia_page(url, session)\n",
    "\n",
    "        if elapsed_time:\n",
    "            times.append(elapsed_time)\n",
    "            if len(times) > 10:  # Keep a rolling average over the last 10 requests\n",
    "                times.pop(0)\n",
    "\n",
    "        # Estimate time remaining\n",
    "        avg_time = sum(times) / len(times) if times else 0\n",
    "        remaining_time = avg_time * (len(urls_to_scrape) - idx - 1)\n",
    "        eta = time.strftime(\"%H:%M:%S\", time.gmtime(remaining_time))\n",
    "\n",
    "        # Update progress bar with ETA instead of printing new lines\n",
    "        progress_bar.set_postfix({\"ETA\": eta})\n",
    "\n",
    "        # Store results properly using .loc\n",
    "        df.loc[urls_to_scrape.index[idx], output_column] = html\n",
    "\n",
    "    return df\n",
    "\n",
    "df[\"wiki_page_html\"] = pd.NA\n",
    "df = scrape_wikipedia_pages(df, \"wiki_page\", \"wiki_page_html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"wiki_scraped_data.csv\", index=False)\n",
    "df = pd.read_csv(\"wiki_scraped_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_raw_box_office_text(html):\n",
    "    \"\"\"Extract the raw box office revenue text from stored Wikipedia HTML.\"\"\"\n",
    "    if pd.isna(html):\n",
    "        return None  # Skip missing HTML\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Find the box office section in the Wikipedia infobox\n",
    "        infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "        if not infobox:\n",
    "            return None  # No infobox found\n",
    "\n",
    "        for row in infobox.find_all(\"tr\"):\n",
    "            header = row.find(\"th\")\n",
    "            if header and \"Box office\" in header.text:\n",
    "                value = row.find(\"td\").text.strip()\n",
    "                return value  # Store raw text without conversion\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting box office data: {e}\")\n",
    "\n",
    "    return None  # Return None if no valid box office data was found\n",
    "\n",
    "def extract_box_office_single_threaded(df, html_column, output_column):\n",
    "    \"\"\"Extract box office data from HTML with progress tracking (single-threaded).\"\"\"\n",
    "    mask = df[output_column].isna()  # Only process rows where output is missing\n",
    "    html_data = df.loc[mask, html_column]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for html in tqdm(html_data, total=len(html_data), desc=\"Extracting Box Office\", unit=\"page\"):\n",
    "        results.append(extract_raw_box_office_text(html))\n",
    "\n",
    "    df.loc[mask, output_column] = results  # Store extracted values\n",
    "    return df\n",
    "\n",
    "# Run the function with multithreading\n",
    "df[\"box_office_wiki_mined\"] = pd.NA\n",
    "df = extract_box_office_single_threaded(df, \"wiki_page_html\", \"box_office_wiki_mined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_box_office_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan  # Non-string inputs become NaN\n",
    "\n",
    "    # Normalize whitespace: replace HTML breaks with a space and collapse whitespace.\n",
    "    text = text.replace(\"<br>\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Process only if there is a US dollar sign with a digit after it.\n",
    "    if not re.search(r\"\\$\\d\", text):\n",
    "        return np.nan\n",
    "\n",
    "    # Regex explanation:\n",
    "    # - \\$: literal dollar sign.\n",
    "    # - (?P<number>(?:\\d{1,3}(?:,\\d{3})+|\\d+(?:\\.\\d+)?)):\n",
    "    #       Either a properly comma-formatted number (e.g. \"13,747,138\") or a plain number (e.g. \"6.3\" or \"4\").\n",
    "    # - (?:\\s*[–-]\\s*(?P<number2>(?:\\d{1,3}(?:,\\d{3})+|\\d+(?:\\.\\d+)?)))?:\n",
    "    #       Optionally capture a range (e.g. \"25.1-29.6\") but we will use only the first number.\n",
    "    # - (?=[^\\d,\\.]|$):\n",
    "    #       Ensure that we stop matching once digits (or commas/periods) that might be part of a second, concatenated number appear.\n",
    "    # - (?:\\s*(?P<scale>million|millon|billion))?:\n",
    "    #       Optionally capture a scale word (accepting “millon” as a typo).\n",
    "    pattern = re.compile(\n",
    "        r\"\\$(?P<number>(?:\\d{1,3}(?:,\\d{3})+|\\d+(?:\\.\\d+)?))\"\n",
    "        r\"(?:\\s*[–-]\\s*(?P<number2>(?:\\d{1,3}(?:,\\d{3})+|\\d+(?:\\.\\d+)?)))?\"\n",
    "        r\"(?=[^\\d,\\.]|$)\"\n",
    "        r\"(?:\\s*(?P<scale>million|millon|billion))?\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    match = pattern.search(text)\n",
    "    if not match:\n",
    "        return np.nan\n",
    "\n",
    "    # Always take the first number if a range is present.\n",
    "    amount_str = match.group(\"number\")\n",
    "    scale = match.group(\"scale\")\n",
    "\n",
    "    # Remove commas for conversion\n",
    "    amount_str = amount_str.replace(',', '')\n",
    "\n",
    "    try:\n",
    "        amount = float(amount_str)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "    # Adjust for scale if present.\n",
    "    if scale:\n",
    "        scale = scale.lower()\n",
    "        if scale in (\"million\", \"millon\"):\n",
    "            amount *= 1_000_000\n",
    "        elif scale == \"billion\":\n",
    "            amount *= 1_000_000_000\n",
    "\n",
    "    return amount\n",
    "\n",
    "df[\"cleaned_box_office_wiki_mined\"] = df[\"box_office_wiki_mined\"].apply(clean_raw_box_office_text)\n",
    "\n",
    "df[\"box_office\"] = df[\"cleaned_box_office_wiki_mined\"].fillna(df[\"box_office\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_office_cols = [\"box_office\", \"box_office_wiki_mined\", \"cleaned_box_office_wiki_mined\"]\n",
    "\n",
    "df[box_office_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## Drop row where Box Office is still missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True, subset=[\"box_office\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"wip_data.csv\", index=False)\n",
    "df = pd.read_csv(\"wip_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "### Writer, Director, Distributor, SoundMix, budget, runtime, language, cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_primary_writer(html):\n",
    "    \"\"\"Extract the primary writer's name from stored Wikipedia HTML.\"\"\"\n",
    "    if pd.isna(html):\n",
    "        return None  # Skip missing HTML\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        # Find the infobox in the Wikipedia HTML\n",
    "        infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "        if not infobox:\n",
    "            return None  # No infobox found\n",
    "\n",
    "        for row in infobox.find_all(\"tr\"):\n",
    "            header = row.find(\"th\")\n",
    "            if header and \"Written by\" in header.text:\n",
    "                writer_cell = row.find(\"td\")\n",
    "                if writer_cell:\n",
    "                    # Try to extract the first linked name (if available)\n",
    "                    first_link = writer_cell.find(\"a\")\n",
    "                    if first_link:\n",
    "                        return first_link.text.strip()\n",
    "\n",
    "                    # If no links, get the first text entry\n",
    "                    return writer_cell.text.strip().split(\",\")[0]  # Take only the first name if comma-separated\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting writer data: {e}\")\n",
    "\n",
    "    return None  # Return None if no valid writer data was found\n",
    "\n",
    "def extract_primary_writer_single_threaded(df, html_column, output_column):\n",
    "    \"\"\"Extract the primary writer data from HTML with progress tracking (single-threaded).\"\"\"\n",
    "    mask = df[output_column].isna()  # Only process rows where output is missing\n",
    "    html_data = df.loc[mask, html_column]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for html in tqdm(html_data, total=len(html_data), desc=\"Extracting Primary Writer\", unit=\"page\"):\n",
    "        results.append(extract_primary_writer(html))\n",
    "\n",
    "    df.loc[mask, output_column] = results  # Store extracted values\n",
    "    return df\n",
    "\n",
    "# Run the function for writers\n",
    "df[\"primary_writer_wiki_mined\"] = pd.NA\n",
    "df = extract_primary_writer_single_threaded(df, \"wiki_page_html\", \"primary_writer_wiki_mined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"writer\"] = df[\"primary_writer_wiki_mined\"].fillna(df[\"writer\"])\n",
    "# df.drop([\"primary_writer_wiki_mined\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_movie_info(html):\n",
    "    \"\"\"Extract movie metadata (director, distributor, budget, runtime, language, top 5 cast) from Wikipedia HTML.\"\"\"\n",
    "    if pd.isna(html):\n",
    "        return None, None, None, None, None, None  # Return all fields as None for missing HTML\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        infobox = soup.find(\"table\", class_=\"infobox\")\n",
    "        if not infobox:\n",
    "            return None, None, None, None, None, None  # No infobox found\n",
    "\n",
    "        data = {\n",
    "            \"director\": None,\n",
    "            \"distributor\": None,\n",
    "            \"budget\": None,\n",
    "            \"runtime\": None,\n",
    "            \"language\": None,\n",
    "            \"top_5_cast\": None\n",
    "        }\n",
    "\n",
    "        for row in infobox.find_all(\"tr\"):\n",
    "            header = row.find(\"th\")\n",
    "            if not header:\n",
    "                continue\n",
    "\n",
    "            key = header.text.strip()\n",
    "            value_cell = row.find(\"td\")\n",
    "            if not value_cell:\n",
    "                continue\n",
    "\n",
    "            # Extract text values\n",
    "            if key == \"Directed by\":\n",
    "                first_director = value_cell.find(\"a\")  # Prefer linked names\n",
    "                data[\"director\"] = first_director.text.strip() if first_director else value_cell.text.strip()\n",
    "\n",
    "            elif key == \"Distributed by\":\n",
    "                distributors = [a.text.strip() for a in value_cell.find_all(\"a\")]  # Get all linked names\n",
    "                if not distributors:\n",
    "                    distributors = [value_cell.text.strip()]  # Fallback to plain text\n",
    "                data[\"distributor\"] = \", \".join(distributors)  # Store as a comma-separated string\n",
    "\n",
    "            elif key == \"Budget\":\n",
    "                data[\"budget\"] = value_cell.text.strip()\n",
    "\n",
    "            elif key == \"Running time\":\n",
    "                data[\"runtime\"] = value_cell.text.strip()\n",
    "\n",
    "            elif key == \"Language\":\n",
    "                languages = [a.text.strip() for a in value_cell.find_all(\"a\")]\n",
    "                if not languages:\n",
    "                    languages = [value_cell.text.strip()]\n",
    "                data[\"language\"] = \", \".join(languages)\n",
    "\n",
    "            elif key == \"Starring\":\n",
    "                cast = [a.text.strip() for a in value_cell.find_all(\"a\")]  # Get all linked cast members\n",
    "                if not cast:\n",
    "                    cast = value_cell.text.strip().split(\"\\n\")  # Fallback for unlinked names\n",
    "                data[\"top_5_cast\"] = \", \".join(cast[:5])  # Limit to the top 5\n",
    "\n",
    "        return (data[\"director\"], data[\"distributor\"], data[\"budget\"],\n",
    "                data[\"runtime\"], data[\"language\"], data[\"top_5_cast\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting movie data: {e}\")\n",
    "\n",
    "    return None, None, None, None, None, None\n",
    "\n",
    "def extract_movie_data(df, html_column):\n",
    "    \"\"\"Extract multiple metadata fields from Wikipedia HTML (single pass).\"\"\"\n",
    "    mask = df[\"director_mined\"].isna()  # Only process rows where output is missing\n",
    "    html_data = df.loc[mask, html_column]\n",
    "\n",
    "    results = []\n",
    "    for html in tqdm(html_data, total=len(html_data), desc=\"Extracting Movie Data\", unit=\"page\"):\n",
    "        results.append(extract_movie_info(html))\n",
    "\n",
    "    # Store results in corresponding DataFrame columns with '_mined' suffix\n",
    "    df.loc[mask, [\"director_mined\", \"distributor_mined\", \"budget_mined\",\n",
    "                  \"runtime_mined\", \"language_mined\", \"cast_mined\"]] = results\n",
    "    return df\n",
    "\n",
    "# Initialize new columns with '_mined' suffix\n",
    "df[[\"director_mined\", \"distributor_mined\", \"budget_mined\",\n",
    "    \"runtime_mined\", \"language_mined\", \"cast_mined\"]] = pd.NA\n",
    "\n",
    "# Run the function\n",
    "df = extract_movie_data(df, \"wiki_page_html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"wip_data.csv\", index=False)\n",
    "#df = pd.read_csv(\"wip_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)\n",
    "\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cast\"] = df[\"cast_mined\"].fillna(df[\"cast\"])\n",
    "# df.drop([\"cast_mined\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"language\"] = df[\"language_mined\"].fillna(df[\"language\"])\n",
    "# df.drop([\"language_mined\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"director\"] = df[\"director_mined\"].fillna(df[\"director\"])\n",
    "# df.drop([\"director_mined\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"distributor_mined\"] = df[\"distributor_mined\"].str.replace(r\"\\[\\d+\\]\", \"\", regex=True).str.strip()\n",
    "df[\"distributor\"] = df[\"distributor_mined\"].fillna(df[\"distributor\"])\n",
    "# df.drop([\"distributor_mined\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_budget_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan  # Return NaN for non-string inputs\n",
    "\n",
    "    # Remove Wikipedia-style citations like [1], [2]\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "\n",
    "    # Handle \"or\" cases by taking the first amount (e.g., \"$1 million or 2 million\")\n",
    "    text = re.split(r\"\\s+or\\s+\", text, maxsplit=1)[0]\n",
    "\n",
    "    # Extract dollar amount\n",
    "    match = re.search(r\"\\$([\\d,.]+(?:\\s?-\\s?[\\d,.]+)?)(?:\\s*(million|billion))?\", text, re.IGNORECASE)\n",
    "    if not match:\n",
    "        return np.nan  # Return NaN if no valid dollar amount is found\n",
    "\n",
    "    amount, scale = match.groups()\n",
    "    amount = amount.replace(',', '')  # Remove commas from numbers\n",
    "\n",
    "    # Handle ranges (e.g., \"1.3-1.6 million\" → take the first number)\n",
    "    if '-' in amount:\n",
    "        amount = amount.split('-')[0]\n",
    "\n",
    "    try:\n",
    "        amount = float(amount)  # Convert to float\n",
    "    except ValueError:\n",
    "        return np.nan  # If conversion fails, return NaN\n",
    "\n",
    "    # Convert million/billion to raw numbers\n",
    "    if scale:\n",
    "        scale = scale.lower()\n",
    "        if scale == \"million\":\n",
    "            amount *= 1_000_000\n",
    "        elif scale == \"billion\":\n",
    "            amount *= 1_000_000_000\n",
    "\n",
    "    return amount\n",
    "\n",
    "df[\"budget_mined\"] = df[\"budget_mined\"].apply(clean_raw_budget_text)\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"budget\"] = df[\"budget_mined\"].fillna(df[\"budget\"])\n",
    "# df.drop([\"budget_mined\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_runtime_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return np.nan  # Return NaN for non-string inputs\n",
    "\n",
    "    # Remove Wikipedia-style citations like [1], [2]\n",
    "    text = re.sub(r\"\\[\\d+\\]\", \"\", text).strip()\n",
    "\n",
    "    # Extract numeric runtime\n",
    "    match = re.search(r\"(\\d+)\", text)\n",
    "    if match:\n",
    "        return float(match.group(1))  # Convert to float\n",
    "\n",
    "    return np.nan  # Return NaN if no valid number is found\n",
    "df[\"runtime_mined\"] = df[\"runtime_mined\"].apply(clean_runtime_text)\n",
    "\n",
    "df[\"runtime\"] = df[\"runtime_mined\"].fillna(df[\"runtime\"])\n",
    "# df.drop([\"runtime_mined\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"wiki_page_html\"], inplace=True, axis=1)\n",
    "display(df)\n",
    "\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_rating_column(value):\n",
    "    if isinstance(value, str):\n",
    "        try:\n",
    "            value = ast.literal_eval(value)  # Convert string representation of list to an actual list\n",
    "            if isinstance(value, list):\n",
    "                return \", \".join(value)  # Join list elements with a comma\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass\n",
    "    return value  # Return as is if not a valid list\n",
    "\n",
    "df[\"rating_contents\"] = df[\"rating_contents\"].apply(clean_rating_column)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x: x.str.lower() if x.dtype == \"object\" else x)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"language\"].str.contains(\"English\", case=False, na=False)]\n",
    "display(df)\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"release_year\"] >= 1977]\n",
    "display(df)\n",
    "display(column_stats(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted index-related columns if they exist\n",
    "df = df.loc[:, ~df.columns.str.match(r'level_0|index')]\n",
    "df.drop(columns=[\"box_office_wiki_mined\", \"cleaned_box_office_wiki_mined\", \"primary_writer_wiki_mined\", \"director_mined\", \"distributor_mined\", \"budget_mined\", \"runtime_mined\", \"language_mined\", \"cast_mined\"], inplace=True)\n",
    "# Reset the index properly\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"final_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
